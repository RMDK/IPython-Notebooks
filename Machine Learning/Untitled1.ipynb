{
 "metadata": {
  "name": "",
  "signature": "sha256:5e0cefef2fa35eb8a04b427e18b679f491b635e2ca7139c6371e8834437ee0ec"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Classification"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Continuation: detecting poor answers"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This notebook is a continuation of [this notebook on classification](http://nbviewer.ipython.org/gist/RMDK/7c54600b9c2af68914b3), where we clustered related Q&A blog posts for a recommendation engine. The next step is building a classifier on this type of data. A challenge of Q&A site admins is maintaining a decent level of quality in the post content. Higher quality post content ultimately results in more users. \n",
      "\n",
      "One way to encourage quality content is allow the question asker to flag one answer to their question as the accepted answer (this is how `stackoverflow` works. This results in more score points for the `asker` and `answer(er)`. \n",
      "\n",
      "What if instead the website continuously evaluated the answer submissions in-progress and provide feedback as to whether the answer shows signs of being inadequate. For example, providing code output, images, and a certain word count.\n",
      "\n",
      "To achieve this we will need to tune the data and classifier extensively, which I will walk-through (at least the basic structure) in this notebook."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Grab the data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The team behind `stackoverflow` provides much of the data under a CC Wiki license. The latest data dump can be found [here](https://archive.org/details/stackexchange). The dataset we need is the `posts.xml`. This dataset contains the question and answer post content."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}