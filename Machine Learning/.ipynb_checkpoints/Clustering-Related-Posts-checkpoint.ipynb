{
 "metadata": {
  "gist_id": "7c54600b9c2af68914b3",
  "name": "",
  "signature": "sha256:805805d0ec6a6ddd3e57016c98fadafda6b2b13c6e575ebe4d84d247e501bb41"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Clustering Related Posts"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This is a series of notebooks (in progress) to document my learning, and hopefully to help others learn machine learning. I would love suggestions / corrections / feedback for these notebooks.\n",
      "\n",
      "<a target=\"_parent\" href=\"http://rmdk.ca\">Visit my webpage for more</a>. \n",
      "\n",
      "Email me: <a target=\"_parent\" href=\"http://rmdk.ca/contact/\">ryan@rmdk.ca</a>\n",
      "\n",
      "I'd love for you to share if you liked this post."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "social()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "\n",
        "    <a style='float:left; margin-right:5px;' href=\"https://twitter.com/share\" class=\"twitter-share-button\" data-text=\"Check this out\" data-via=\"Ryanmdk\">Tweet</a>\n",
        "<script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>\n",
        "    <a style='float:left; margin-right:5px;' href=\"https://twitter.com/Ryanmdk\" class=\"twitter-follow-button\" data-show-count=\"false\">Follow @Ryanmdk</a>\n",
        "<script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>\n",
        "    <a style='float:left; margin-right:5px;'target='_parent' href=\"http://www.reddit.com/submit\" onclick=\"window.location = 'http://www.reddit.com/submit?url=' + encodeURIComponent(window.location); return false\"> <img src=\"http://www.reddit.com/static/spreddit7.gif\" alt=\"submit to reddit\" border=\"0\" /> </a>\n",
        "<script src=\"//platform.linkedin.com/in.js\" type=\"text/javascript\">\n",
        "  lang: en_US\n",
        "</script>\n",
        "<script type=\"IN/Share\"></script>\n"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 123,
       "text": [
        "<IPython.core.display.HTML at 0x11f0b52d0>"
       ]
      }
     ],
     "prompt_number": 123
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "This notebook covers or includes:   "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Introduction to word processing\n",
      "* Natural Language Learning Toolkit "
     ]
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "TO DO:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Include measures of model fit and error\n",
      "* Finish"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Measuring Similarity Between Text Messages:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "This notebook will explore the idea of recommending news posts to a reader based their search query. To do this, we also have to introduce basic text processing. Clustering can be defined as classifying unlabelled data by a measurement of similarity.\n",
      "\n",
      "One of the most robust methods to quantify meaning in textual data is using the **bag-of-word** approach. For each word in the post, we count track the number of occurances in a vector (vectorization). In this way the data can be stored in an efficient matrix structure."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Preprocessing:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First we have to convert the text into a `bag-of-words`. We can do this using scikit's builtin `CountVectorizer`. The input `min_df` determines how the function will treat words that are used infrequently. If set to an interger, all words occuring less than that amount will be dropped. If set to a fraction, all words that occur less than the fraction of the overall dataset will be dropped. There are also a lot of other options which will we get into later."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "\n",
      "vect = CountVectorizer(min_df=1)\n",
      "print vect"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "CountVectorizer(analyzer=word, binary=False, charset=None, charset_error=None,\n",
        "        decode_error=strict, dtype=<type 'numpy.int64'>, encoding=utf-8,\n",
        "        input=content, lowercase=True, max_df=1.0, max_features=None,\n",
        "        min_df=1, ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
        "        strip_accents=None, token_pattern=(?u)\\b\\w\\w+\\b, tokenizer=None,\n",
        "        vocabulary=None)\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We see that for now the counting is done at the word level (`analyzer = word`)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "content = ['how to open a beer without a bottle opener', \n",
      "           'Beer bottles or beer cans',]\n",
      "X = vect.fit_transform(content)\n",
      "\n",
      "vect.get_feature_names()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 29,
       "text": [
        "[u'beer', u'bottle', u'bottles', u'cans', u'open', u'opener']"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Print the vectorized word occurances\n",
      "print X\n",
      "print X.toarray()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  (0, 0)\t1\n",
        "  (1, 0)\t2\n",
        "  (0, 1)\t1\n",
        "  (1, 2)\t1\n",
        "  (1, 3)\t1\n",
        "  (0, 4)\t1\n",
        "  (0, 5)\t1\n",
        "[[1 1 0 0 1 1]\n",
        " [2 0 1 1 0 0]]\n"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- Count vectors returned by `transform` are stored in the more memory efficient coordinate matrix format, we have to access the full standard vector for analysis though. \n",
      "\n",
      "Let's add some more data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "posts = ['how to open a beer without a bottle opener', \n",
      "           'Do girls like beer bottles or beer cans?',\n",
      "           'where did all my beer go?',\n",
      "           'where did all my beer go? where did all my beer go?',\n",
      "           'recycling beer bottles and cans',\n",
      "           'Is it worth recycling?',\n",
      "           'do not bring bottles to my backyard party, only cans please.', \n",
      "           'This is useless']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train = vect.fit_transform(posts)\n",
      "\n",
      "num_samples, num_features = X_train.shape\n",
      "\n",
      "print '#samples: {}, #features: {}'.format(num_samples, num_features)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "#samples: 8, #features: 15\n"
       ]
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- Unsurprisingly, we have 8 posts with a total of 15 different words. Now we can vectorize our data.\n",
      "\n",
      "Let's vectorize a new post, then see how similar it is to our existing corpus."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "new_post = 'Opening beer bottles and cans 101'\n",
      "new_post_vect = vect.transform([new_post])\n",
      "\n",
      "print(new_post_vect).toarray()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[0 1 0 1 0 1 0 0 0 0 0 0 0 0 0]]\n"
       ]
      }
     ],
     "prompt_number": 36
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import scipy as sp\n",
      "\n",
      "def dists(v1, v2):\n",
      "    delta = v1-v2\n",
      "    # Calculate Euclidean \"norm\" distance\n",
      "    return sp.linalg.norm(delta.toarray())\n",
      "\n",
      "import sys\n",
      "\n",
      "def similarity(new_post_vector, corpus):\n",
      "    best_dist = 999\n",
      "    best_i = None\n",
      "    \n",
      "    for i in xrange(len(corpus.toarray())):\n",
      "        post = posts[i]\n",
      "        \n",
      "        if post == new_post:\n",
      "            continue\n",
      "        post_vec = corpus.getrow(i)\n",
      "        d = dists(post_vec, new_post_vector)\n",
      "        print 'Post %i with dist = %.2f: %s'%(i, d, post)\n",
      "        \n",
      "        if d < best_dist:\n",
      "            best_dist = d\n",
      "            best_i = i\n",
      "    print 'Best post is {} with dist = {}'.format(best_i, best_dist)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 37
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "similarity(new_post_vect, X_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Post 0 with dist = 2.24: how to open a beer without a bottle opener\n",
        "Post 1 with dist = 1.73: Do girls like beer bottles or beer cans?\n",
        "Post 2 with dist = 1.73: where did all my beer go?\n",
        "Post 3 with dist = 2.65: where did all my beer go? where did all my beer go?\n",
        "Post 4 with dist = 1.00: recycling beer bottles and cans\n",
        "Post 5 with dist = 2.24: Is it worth recycling?\n",
        "Post 6 with dist = 2.00: do not bring bottles to my backyard party, only cans please.\n",
        "Post 7 with dist = 2.00: This is useless\n",
        "Best post is 4 with dist = 1.0\n"
       ]
      }
     ],
     "prompt_number": 38
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Great, our first text similarity measurement! We can see here that post 3 is most similar to our new post. However, we can see that `post 2` is \"closer\" to `post 3`, even though `post 3` is simply `post 2` doubled. It is clear the simple counts of words is too simple. The next step is to normalize the word counts to get vectors of unitless lengths to avoid this problem."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Update our dists function\n",
      "def dists(v1, v2):\n",
      "    v1_norm = v1/sp.linalg.norm(v1.toarray())\n",
      "    v2_norm = v2/sp.linalg.norm(v2.toarray())\n",
      "    delta = v1_norm-v2_norm\n",
      "    # Calculate Euclidean \"norm\" distance\n",
      "    return sp.linalg.norm(delta.toarray())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 39
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "similarity(new_post_vect, X_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Post 0 with dist = 1.19: how to open a beer without a bottle opener\n",
        "Post 1 with dist = 0.61: Do girls like beer bottles or beer cans?\n",
        "Post 2 with dist = 1.09: where did all my beer go?\n",
        "Post 3 with dist = 1.09: where did all my beer go? where did all my beer go?\n",
        "Post 4 with dist = 0.52: recycling beer bottles and cans\n",
        "Post 5 with dist = 1.41: Is it worth recycling?\n",
        "Post 6 with dist = 0.98: do not bring bottles to my backyard party, only cans please.\n",
        "Post 7 with dist = 1.41: This is useless\n",
        "Best post is 4 with dist = 0.517638090205\n"
       ]
      }
     ],
     "prompt_number": 40
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Great, posts 2 & 3 are now equally similar to our new post."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Removing Less Important Words:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There are many words in language that do not carry much meaning in terms of the overall interpretation of the message. Words like \"it\" should be much less meaningful than \"beer\" in our current context. These less important words are called `stop words`, and can be removed from the posts since they do not help us distiguish between different posts."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Add english stop words to our vectorizer object.\n",
      "vect = CountVectorizer(min_df=1, stop_words='english')\n",
      "#Display a sample\n",
      "print sorted(vect.get_stop_words())[80:-150]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['empty', 'enough', 'etc', 'even', 'ever', 'every', 'everyone', 'everything', 'everywhere', 'except', 'few', 'fifteen', 'fify', 'fill', 'find', 'fire', 'first', 'five', 'for', 'former', 'formerly', 'forty', 'found', 'four', 'from', 'front', 'full', 'further', 'get', 'give', 'go', 'had', 'has', 'hasnt', 'have', 'he', 'hence', 'her', 'here', 'hereafter', 'hereby', 'herein', 'hereupon', 'hers', 'herself', 'him', 'himself', 'his', 'how', 'however', 'hundred', 'i', 'ie', 'if', 'in', 'inc', 'indeed', 'interest', 'into', 'is', 'it', 'its', 'itself', 'keep', 'last', 'latter', 'latterly', 'least', 'less', 'ltd', 'made', 'many', 'may', 'me', 'meanwhile', 'might', 'mill', 'mine', 'more', 'moreover', 'most', 'mostly', 'move', 'much', 'must', 'my', 'myself', 'name']\n"
       ]
      }
     ],
     "prompt_number": 41
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If you already have a list of words in mind you with to `stop`, you can simply pass them as a list to the `stop_words` argument."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Stemming"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We also need to consider that similar words, such as \"girl\" and \"girls\" should probably be considered as the same word. Thus we need a function that reduces words to a finite 'word stem'. We can do thsi with the **Natural Language Toolkit (NLTK)**. After installing NLTK, import the library and try out the stemmer for english."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk.stem\n",
      "\n",
      "s = nltk.stem.SnowballStemmer('english')\n",
      "\n",
      "print s.stem('bottles')\n",
      "print s.stem('bottle')\n",
      "\n",
      "print s.stem('perception')\n",
      "print s.stem('perceptive')\n",
      "\n",
      "print s.stem('crashing')\n",
      "print s.stem('crashed')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "bottl\n",
        "bottl\n",
        "percept\n",
        "percept\n",
        "crash\n",
        "crash\n"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Extending the vectorizer with NLTK stemming"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We need to step the posts before we feed then into the `CountVectorizer`. The best way to do this is overwrite the method `build_analyzer`. \n",
      "\n",
      "By doing this we utilize the preprocessing functions in the parent class that converts the raw posts into lower case. We tokenize all the words, and then convert each word into the stemmed version."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk.stem\n",
      "\n",
      "english_stemmer = nltk.stem.SnowballStemmer('english')\n",
      "\n",
      "class StemmedCountVectorizer(CountVectorizer):\n",
      "    def build_analyzer(self):\n",
      "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
      "        return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))\n",
      "    \n",
      "vectorizer = StemmedCountVectorizer(min_df=1, stop_words='english')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = vectorizer.fit_transform(posts)\n",
      "vectorizer.get_feature_names()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 27,
       "text": [
        "[u'backyard',\n",
        " u'beer',\n",
        " u'bottl',\n",
        " u'bring',\n",
        " u'can',\n",
        " u'did',\n",
        " u'girl',\n",
        " u'like',\n",
        " u'open',\n",
        " u'parti',\n",
        " u'recycl',\n",
        " u'useless',\n",
        " u'worth']"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Restate the new vectorizer on the data\n",
      "X_train = vectorizer.fit_transform(posts)\n",
      "new_post_vect = vectorizer.transform([new_post])\n",
      "\n",
      "similarity(new_post_vect, X_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Post 0 with dist = 0.61: how to open a beer without a bottle opener\n",
        "Post 1 with dist = 0.77: Do girls like beer bottles or beer cans?\n",
        "Post 2 with dist = 1.14: where did all my beer go?\n",
        "Post 3 with dist = 1.14: where did all my beer go? where did all my beer go?\n",
        "Post 4 with dist = 0.71: recycling beer bottles and cans\n",
        "Post 5 with dist = 1.41: Is it worth recycling?\n",
        "Post 6 with dist = 1.05: do not bring bottles to my backyard party, only cans please.\n",
        "Post 7 with dist = 1.41: This is useless\n",
        "Best post is 0 with dist = 0.605810893055\n"
       ]
      }
     ],
     "prompt_number": 48
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We see now that post 0 is most similar to our new post, because bottles and bottle are now treated as the same word."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print new_post\n",
      "print posts[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Opening beer bottles and cans 101\n",
        "how to open a beer without a bottle opener\n"
       ]
      }
     ],
     "prompt_number": 51
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Thinking a bit deeper about relevant post features"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So far we have considered that higher occurrence of certains words in post equates to a greater importance of that word in the post. While this is true to some extent, there is the case where very frequent words really don't carry any meaning to posts. For example, the word \"Subject\" appears in every blog post, thus it is not really communicating anything important, and does not help us distinguish between posts.\n",
      "\n",
      "We could perhaps set a 90% occurrence cutoff in our tokenizer, such that words that occur in >90% of the posts are excluded, however, we still run into the problem of border cases, say where the word occurs in only 89% of the posts.\n",
      "\n",
      "To solve these problems we count the term frequencies for every post **while** discounting those words that appear in many posts. This is the concept of **term frequency - inverse document frequency (TF-IDF)**. We can implement this using scikit learn's `TfidfVectorizer`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "# Rebuild the function to include our stemmer\n",
      "\n",
      "class StemmedTfidfVectorizer(TfidfVectorizer):\n",
      "    def build_analyzer(self):\n",
      "        analyzer = super(TfidfVectorizer, self).build_analyzer()\n",
      "        \n",
      "        return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))\n",
      "\n",
      "vectorizer = StemmedTfidfVectorizer(min_df=1, stop_words='english', decode_error='ignore')\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 60
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now instead of counts, our document vectors will contain individual TF-IDF values per term (token)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Restate new vectorizer\n",
      "X_train = vectorizer.fit_transform(posts)\n",
      "new_post_vect = vectorizer.transform([new_post])\n",
      "\n",
      "similarity(new_post_vect, X_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Post 0 with dist = 0.57: how to open a beer without a bottle opener\n",
        "Post 1 with dist = 0.99: Do girls like beer bottles or beer cans?\n",
        "Post 2 with dist = 1.26: where did all my beer go?\n",
        "Post 3 with dist = 1.26: where did all my beer go? where did all my beer go?\n",
        "Post 4 with dist = 0.90: recycling beer bottles and cans\n",
        "Post 5 with dist = 1.41: Is it worth recycling?\n",
        "Post 6 with dist = 1.17: do not bring bottles to my backyard party, only cans please.\n",
        "Post 7 with dist = 1.41: This is useless\n",
        "Best post is 0 with dist = 0.572957858071\n"
       ]
      }
     ],
     "prompt_number": 63
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Recap"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So far we have:\n",
      "\n",
      "1. Tokenized text\n",
      "2. Discard words that occur too often and don't help us detect relevant posts\n",
      "3. Throw away very uncommon words\n",
      "4. Count the remaining words\n",
      "5. Calculated TF-IDF values from the counts, considering the whole text corpus.\n",
      "\n",
      "**Limitations of the bag-of-words approach**\n",
      "\n",
      "* It does not cover word relations: \"Car hits wall\" and \"Wall hits car\" will both have the same feature vector.\n",
      "* It does not count negations well: \"I will eat soup\" and \"I will *not* eat soup\" will have very similar feature vectors. Though this can be remedied by also counting bigrams and trigrams (two or three words in a row together).\n",
      "* Totally fails with misspelled words."
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Clustering"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that we can represent our blog posts quantitatively, to some degree. Now our goal is to cluster similar posts. There are two main times of clustering algorithms: **flat** and **hierarchical**. \n",
      "\n",
      "**Flat clustering** divides the posts into sets of clusters that minimizes the difference _within_ clusters and maximized the difference _between_ clusters. Generally we have to specify the number of clusters upfront.\n",
      "\n",
      "**Hierarchical clustering** does not require the number of clusters as an input. It creates a hierarchy of clusters where very similar posts are grouped together, then similar clusters are then further grouped recursively until one cluster is left that contains all the data. Once completed, the user can discern the optimal number of clusters."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "KMeans"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "KMeans is probably the most common **flat** clustering algorithm. First you must specify the number of desired clusters (k). From there, the algorithm first specifies k random _seeds_ within the data. Then it assigns each post to the closest seed centroid. Next, the seeds are relocated to the mean center of the points initially assigned to it. Then the process is repeat, whereby the posts are then reassigned based on the new closest seed point. This continues as long as the seed centroids move a considerable amount, after some _n_ iterations, the movements will fall below a threshold. The algorithm is then considered converged."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Get some test data "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We will utilize a machine learning dataset that contains 18 826 posts from 20 different newsgroups. There are many topics including technology, politics, and religion. However, for now we will only use the technical groups.\n",
      "\n",
      "One question we could ask is, for a certain topic, can we effectivly cluster the newgroups who published that topic into distinct categories?\n",
      "\n",
      "\n",
      "This data is already split into testing and training data, we can download the data using sklearn."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sklearn.datasets\n",
      "\n",
      "save_dir = '/users/ryankelly/downloads/' # Your save file path\n",
      "\n",
      "# Download data using sklearn\n",
      "df = sklearn.datasets.load_mlcomp(\"20news-18828\", mlcomp_root=save_dir)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 87
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Data files\n",
      "print df.filenames\n",
      "print len(df.filenames)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['/users/ryankelly/downloads/379/raw/comp.graphics/1190-38614'\n",
        " '/users/ryankelly/downloads/379/raw/comp.graphics/1383-38616'\n",
        " '/users/ryankelly/downloads/379/raw/alt.atheism/487-53344' ...,\n",
        " '/users/ryankelly/downloads/379/raw/rec.sport.hockey/10215-54303'\n",
        " '/users/ryankelly/downloads/379/raw/sci.crypt/10799-15660'\n",
        " '/users/ryankelly/downloads/379/raw/comp.os.ms-windows.misc/2732-10871']\n",
        "18828\n"
       ]
      }
     ],
     "prompt_number": 90
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Data Topics\n",
      "df.target_names"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 92,
       "text": [
        "['alt.atheism',\n",
        " 'comp.graphics',\n",
        " 'comp.os.ms-windows.misc',\n",
        " 'comp.sys.ibm.pc.hardware',\n",
        " 'comp.sys.mac.hardware',\n",
        " 'comp.windows.x',\n",
        " 'misc.forsale',\n",
        " 'rec.autos',\n",
        " 'rec.motorcycles',\n",
        " 'rec.sport.baseball',\n",
        " 'rec.sport.hockey',\n",
        " 'sci.crypt',\n",
        " 'sci.electronics',\n",
        " 'sci.med',\n",
        " 'sci.space',\n",
        " 'soc.religion.christian',\n",
        " 'talk.politics.guns',\n",
        " 'talk.politics.mideast',\n",
        " 'talk.politics.misc',\n",
        " 'talk.religion.misc']"
       ]
      }
     ],
     "prompt_number": 92
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Restrict data to only 'tech' categories\n",
      "group = ['comp.graphics', 'comp.os.ms-windows.misc', \n",
      "         'comp.sys.ibm.pc.hardware', 'comp.sys.ma c.hardware', \n",
      "         'comp.windows.x', 'sci.space']\n",
      "# Reload in only training data with the desired categories\n",
      "train_data = sklearn.datasets.load_mlcomp('20news-18828', 'train', \n",
      "                                          mlcomp_root=save_dir, \n",
      "                                          categories=group)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 94
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(len(train_data.filenames))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "3414\n"
       ]
      }
     ],
     "prompt_number": 97
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Clustering posts"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "While initializing our `vectorizer` we have to remember that we are working with real data, which has many errors, which in this case invalid characers that cannot be encoded."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vec = StemmedTfidfVectorizer(min_df=10, max_df=0.5,\n",
      "                              stop_words='english', decode_error='ignore')\n",
      "\n",
      "vecData = vec.fit_transform(train_data.data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 105
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "num_samples, num_features = vecData.shape\n",
      "print('#samples: {}, #features: {}').format(num_samples, num_features)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "#samples: 3414, #features: 4331\n"
       ]
      }
     ],
     "prompt_number": 106
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This is the information we will use as input for KMeans clustering."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "num_clusters = 6\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "km = KMeans(n_clusters=num_clusters, init='random', n_init=1, verbose=1)\n",
      "km.fit(vecData)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Initialization complete\n",
        "Iteration  0, inertia 6359.460\n",
        "Iteration  1, inertia 3293.526\n",
        "Iteration  2, inertia 3274.609\n",
        "Iteration  3, inertia 3268.507\n",
        "Iteration  4, inertia 3264.962"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iteration  5, inertia 3261.204\n",
        "Iteration  6, inertia 3256.584\n",
        "Iteration  7, inertia 3252.783\n",
        "Iteration  8, inertia 3250.599\n",
        "Iteration  9, inertia 3249.219"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iteration 10, inertia 3248.640\n",
        "Iteration 11, inertia 3248.320\n",
        "Iteration 12, inertia 3248.198\n",
        "Iteration 13, inertia 3248.132\n",
        "Iteration 14, inertia 3248.085"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iteration 15, inertia 3248.051\n",
        "Iteration 16, inertia 3248.019\n",
        "Iteration 17, inertia 3248.014\n",
        "Iteration 18, inertia 3248.003\n",
        "Iteration 19, inertia 3248.000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Converged at iteration 19\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 117,
       "text": [
        "KMeans(copy_x=True, init='random', max_iter=300, n_clusters=6, n_init=1,\n",
        "    n_jobs=1, precompute_distances=True, random_state=None, tol=0.0001,\n",
        "    verbose=1)"
       ]
      }
     ],
     "prompt_number": 117
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.core.display import HTML\n",
      "\n",
      "\n",
      "def css_styling():\n",
      "    styles = open(\"/users/ryankelly/desktop/custom_notebook.css\", \"r\").read()\n",
      "\n",
      "    return HTML(styles)\n",
      "css_styling()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "\n",
        "<style>\n",
        "body {\n",
        "    font-family: Century Gothic, sans;\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "div.text_cell_render h1 { /* Main titles bigger, centered */\n",
        "font-size: 2.2em;\n",
        "line-height:1.4em;\n",
        "text-align:center;\n",
        "}\n",
        "\n",
        "/*Input and output cells formatting*/\n",
        "div.prompt.input_prompt, div.prompt.output_prompt {\n",
        "    visibility: hidden;\n",
        "    /*font-family: Consolas;*/\n",
        "    color: #575748;\n",
        "    /*background-color: #CCCCCC;*/\n",
        "    border: 0px;\n",
        "    width: 6.5em;\n",
        "    float:left;\n",
        "}\n",
        "\n",
        "\n",
        "div.output_subarea.output_text.output_stream.output_stdout,div.output_subarea.output_text {\n",
        "    margin-left: 1.5em;\n",
        "    padding-top: 1em;\n",
        "    padding-bottom: 0.5em;\n",
        "    margin-top: 8px; /*This is for getting the box-shadow property of the parent to display properly;*/\n",
        "}\n",
        "\n",
        "div.cell { /* Tunes the space between cells */\n",
        "margin-top:1em;\n",
        "margin-bottom:1em;\n",
        "width:100%;\n",
        "margin-right:auto;\n",
        "overflow-x:hidden;\n",
        "}\n",
        "\n",
        "div.text_cell_render{\n",
        "    overflow-x:hidden;\n",
        "      \n",
        "}\n",
        "\n",
        "\n",
        "div.input{\n",
        "margin-right:1%;\n",
        "}\n",
        "\n",
        "</style>\n",
        " \n",
        "\n",
        "\n",
        "\n"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 122,
       "text": [
        "<IPython.core.display.HTML at 0x11f0b5a10>"
       ]
      }
     ],
     "prompt_number": 122
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def social():\n",
      "    code = \"\"\"\n",
      "    <a style='float:left; margin-right:5px;' href=\"https://twitter.com/share\" class=\"twitter-share-button\" data-text=\"Check this out\" data-via=\"Ryanmdk\">Tweet</a>\n",
      "<script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>\n",
      "    <a style='float:left; margin-right:5px;' href=\"https://twitter.com/Ryanmdk\" class=\"twitter-follow-button\" data-show-count=\"false\">Follow @Ryanmdk</a>\n",
      "<script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>\n",
      "    <a style='float:left; margin-right:5px;'target='_parent' href=\"http://www.reddit.com/submit\" onclick=\"window.location = 'http://www.reddit.com/submit?url=' + encodeURIComponent(window.location); return false\"> <img src=\"http://www.reddit.com/static/spreddit7.gif\" alt=\"submit to reddit\" border=\"0\" /> </a>\n",
      "<script src=\"//platform.linkedin.com/in.js\" type=\"text/javascript\">\n",
      "  lang: en_US\n",
      "</script>\n",
      "<script type=\"IN/Share\"></script>\n",
      "\"\"\"\n",
      "    return HTML(code)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 121
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}