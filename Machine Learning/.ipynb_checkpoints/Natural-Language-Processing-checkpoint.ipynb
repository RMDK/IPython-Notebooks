{
 "metadata": {
  "name": "",
  "signature": "sha256:d3c683a3fa16f31d30d62a2404fdcb69e8fbedec6c13d7db37915365d178d234"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Machine Learning: Natural Language Processing"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This is a series of notebooks (in progress) to document my learning, and hopefully to help others learn machine learning. I would love suggestions / corrections / feedback for these notebooks.\n",
      "\n",
      "<a href=\"http://rmdk.ca\">Visit my webpage for more</a>. \n",
      "\n",
      "Email me: <a href=\"mailto:email.ryan.kelly@gmail.com?Subject=Hey\" target=\"_top\">email.ryan.kelly@gmail.com</a>\n",
      "\n",
      "<hr>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "This notebook covers or includes:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- Introduction to language processing\n",
      "- Gathering Twitter data and pre processing tweets"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Data Sources:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- <a target=\"_parent\" href=\"http://www.nltk.org/book/ch01.html\">nltk book</a>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 6,
     "metadata": {},
     "source": [
      "TO DO:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "I just started, give me a break"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<hr>\n",
      "**Related Notebooks:**\n",
      "- <a target=\"_parent\" href=\"http://rmdk.ca/machine-learning-curve-fitting/\">Curve Fitting</a>\n",
      "- <a target=\"_parent\" href=\"http://rmdk.ca/twitter_analysis_influenza/\">Flu Tracking with Twitter</a>\n",
      "- <a target=\"_parent\" href=\"http://rmdk.ca/numpy-python/\">Numpy quick-start</a>\n",
      "- <a target=\"_parent\" href=\"http://rmdk.ca/classification/\">Classification</a>\n",
      "- <a target=\"_parent\" href=\"http://rmdk.ca/creating-test-data/\">Creating Test Data (Template)</a>\n",
      "\n",
      "In Progress:\n",
      "- Data Cleaning\n",
      "- Regression\n",
      "\n",
      "Coming Soon:\n",
      "- Clustering\n",
      "- Dimension Reduction\n",
      "- Natural Language Processing\n",
      "\n",
      "<hr>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Getting Started:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Generally, raw text is fairly meaningless for a computer to process. Natural language processing involves transforming text into meaningful numbers so that we can leverage machine learning techniques such as clustering, classification, support vector machines, and many others.\n",
      "\n",
      "The preferred approach to quantify text information is the **bag-of-word** technique. For every word in the text body, its occurance is tallied then recorded as a vector in a process known as **vectorization**.\n",
      "\n",
      "Let's start at the beginning:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First we need to load the nltk library, then open the bundled collections of data that they provide for free using `nltk.download()`.\n",
      "Once you enter this command, dowload all the data they provide for the book, as we will be using some of the content here."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "nltk.download()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "showing info http://nltk.github.com/nltk_data/\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "True"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that we have some text to work with, we can import what we downloaded."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.book import *"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "*** Introductory Examples for the NLTK Book ***\n",
        "Loading text1, ..., text9 and sent1, ..., sent9\n",
        "Type the name of the text or sentence to view it.\n",
        "Type: 'texts()' or 'sents()' to list the materials.\n",
        "text1:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Moby Dick by Herman Melville 1851\n",
        "text2:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Sense and Sensibility by Jane Austen 1811\n",
        "text3:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " The Book of Genesis\n",
        "text4:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Inaugural Address Corpus\n",
        "text5:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Chat Corpus\n",
        "text6: Monty Python and the Holy Grail\n",
        "text7:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Wall Street Journal\n",
        "text8: Personals Corpus\n",
        "text9:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " The Man Who Was Thursday by G . K . Chesterton 1908\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 10,
       "text": [
        "<Text: Moby Dick by Herman Melville 1851>"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The most basic commends we can do with text are simple keyword searches. We can do this using the `concordance view`. I think the **chat corpus** (text 5) is the most interesting for now, as it contains ~10 000 chat posts from age-specific chat rooms.\n",
      "\n",
      "The `concordance view` finds the keyword of interest, then shows surrounding sentence to give context.\n",
      "\n",
      "Here we search for text containing the word \"weather\"."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text5.concordance(\"weather\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Displaying 7 of 7 matches:\n",
        " for asking -- can you believe this weather -- Yeah it 's fall !!! hi U46 good \n",
        "to look out the window to check the weather ... haha www.Wunderground.com : . L\n",
        "to look out the window to check the weather ... thanks !!! . Advisory //www.wun\n",
        "end Live Oak , California ( 95953 ) weather ) www.Wunderground.com : . Fairbank\n",
        " ( end Fairbanks , Alaska ( 99701 ) weather ) sigh i am 20 boy PART from azerba\n",
        "to look out the window to check the weather ... www.Wunderground.com : . Aberde\n",
        "d Aberdeen , South Dakota ( 57401 ) weather ) i 'd never leave you alone U35 ..\n"
       ]
      }
     ],
     "prompt_number": 112
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "While these datasets are interesting enough, lets pull in some Twitter data instead!"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Retrieving tweets from Twitter:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First you will need to <a href = \"https://dev.twitter.com/apps\">create an application</a> to retrieve your authentication keys which allow you to access the Twitter API. You will also need to `pip install twitter`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import twitter\n",
      "import json\n",
      "# Load in your application keys\n",
      "CONSUMER_KEY = 'EwFMhx5EOCp7kAyxf5gkfRNAr'\n",
      "CONSUMER_SECRET ='eRKvDIWWMWCWOM1aztw6wK4A9nogJBvDZZBT1QVOtUf2PVMDlI'\n",
      "OAUTH_TOKEN = '1358375179-QXI5GTDTAaS6KytB249aleNEtOQRSMB4vYlvhHA'\n",
      "OAUTH_TOKEN_SECRET = 'MX20qtIAnibJwYrGYVSBBbTQCIr6Ss2Tg3NZs7cBuaHEf'\n",
      "\n",
      "auth = twitter.oauth.OAuth(OAUTH_TOKEN, OAUTH_TOKEN_SECRET,\n",
      "                           CONSUMER_KEY, CONSUMER_SECRET)\n",
      "# Create twitter streaming object\n",
      "twitter_api = twitter.Twitter(auth=auth)\n",
      "\n",
      "print twitter_api"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<twitter.api.Twitter object at 0x1205cd3d0>\n"
       ]
      }
     ],
     "prompt_number": 120
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "q = '#RaptorsDay' \n",
      "\n",
      "count = 100\n",
      "\n",
      "results = twitter_api.search.tweets(q=q, count=count)\n",
      "\n",
      "statuses = results['statuses']\n",
      "\n",
      "\n",
      "# Iterate through 5 more batches of results by following the cursor\n",
      "\n",
      "for _ in range(15):\n",
      "    print \"Length of statuses\", len(statuses)\n",
      "    try:\n",
      "        next_results = results['search_metadata']['next_results']\n",
      "    except KeyError, e: # No more results when next_results doesn't exist\n",
      "        break\n",
      "        \n",
      "    # Create a dictionary from next_results, which has the following form:\n",
      "    kwargs = dict([ kv.split('=') for kv in next_results[1:].split(\"&\") ])\n",
      "    \n",
      "    search_results = twitter_api.search.tweets(**kwargs)\n",
      "    statuses += results['statuses']\n",
      "\n",
      "# Get just the tweets from the data\n",
      "\n",
      "status_texts = [ status['text'] \n",
      "                 for status in statuses ]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Length of statuses 100\n",
        "Length of statuses"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 200\n",
        "Length of statuses"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 400\n",
        "Length of statuses"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 800\n",
        "Length of statuses"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1600\n",
        "Length of statuses"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 3200\n",
        "Length of statuses"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 6400\n",
        "Length of statuses"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 12800\n",
        "Length of statuses"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 25600\n",
        "Length of statuses"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 51200\n",
        "Length of statuses"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 102400\n",
        "Length of statuses"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 204800\n",
        "Length of statuses"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 409600\n",
        "Length of statuses"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 819200\n",
        "Length of statuses"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1638400\n"
       ]
      }
     ],
     "prompt_number": 151
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "\n",
      "tweets = pd.DataFrame(status_texts)\n",
      "\n",
      "tweets.to_csv(\"/users/ryankelly/projects/raptorsday.csv\", sep = \"\\t\", encoding='utf-8')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 155
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>0</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td> beats,instrumentals,lyrics or features for you...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td> @Raptors  I didn't know  it was #RaptorsDay  e...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td> RT @RaptorsDancePak: It's official! May 12th i...</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td> #CityOfToronto needs to get some self-respect....</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td> RT @Raptors: Pardon the glare, but a closer lo...</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "<p>5 rows \u00d7 1 columns</p>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 150,
       "text": [
        "                                                   0\n",
        "0  beats,instrumentals,lyrics or features for you...\n",
        "1  @Raptors  I didn't know  it was #RaptorsDay  e...\n",
        "2  RT @RaptorsDancePak: It's official! May 12th i...\n",
        "3  #CityOfToronto needs to get some self-respect....\n",
        "4  RT @Raptors: Pardon the glare, but a closer lo...\n",
        "\n",
        "[5 rows x 1 columns]"
       ]
      }
     ],
     "prompt_number": 150
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}