{
 "metadata": {
  "gist_id": "7c54600b9c2af68914b3",
  "name": "",
  "signature": "sha256:02a5fcee11f5ba0e21f46f19a227a28347d2555e702bb956b19925b0d687a3c3"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Clustering Related Posts"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This notebook will explore the idea of recommending news posts to a reader based their search query. To do this, we also have to introduce basic text processing.\n",
      "\n",
      "Clustering can be defined as classifying unlabelled data by a measurement of similarity."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Measuring Similarity Between Text Messages:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "One of the most robust methods to quantify meaning in textual data is using the **bag-of-word** approach. For each word in the post, we count track the number of occurances in a vector (vectorization). In this way the data can be stored in an efficient matrix structure."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Preprocessing:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First we have to convert the text into a `bag-of-words`. We can do this using scikit's builtin `CountVectorizer`. The input `min_df` determines how the function will treat words that are used infrequently. If set to an interger, all words occuring less than that amount will be dropped. If set to a fraction, all words that occur less than the fraction of the overall dataset will be dropped. There are also a lot of other options which will we get into later."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "\n",
      "vect = CountVectorizer(min_df=1)\n",
      "print vect"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "CountVectorizer(analyzer=word, binary=False, charset=None, charset_error=None,\n",
        "        decode_error=strict, dtype=<type 'numpy.int64'>, encoding=utf-8,\n",
        "        input=content, lowercase=True, max_df=1.0, max_features=None,\n",
        "        min_df=1, ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
        "        strip_accents=None, token_pattern=(?u)\\b\\w\\w+\\b, tokenizer=None,\n",
        "        vocabulary=None)\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We see that for now, the counting is done at the word level (`analyzer = word`)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "content = ['how to open a beer without a bottle opener', \n",
      "           'Beer bottles or beer cans',]\n",
      "X = vect.fit_transform(content)\n",
      "\n",
      "vect.get_feature_names()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 10,
       "text": [
        "[u'beer',\n",
        " u'bottle',\n",
        " u'bottles',\n",
        " u'cans',\n",
        " u'how',\n",
        " u'open',\n",
        " u'opener',\n",
        " u'or',\n",
        " u'to',\n",
        " u'without']"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Print the vectorized word occurances\n",
      "print X\n",
      "print X.toarray()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "  (0, 0)\t1\n",
        "  (1, 0)\t2\n",
        "  (0, 1)\t1\n",
        "  (1, 2)\t1\n",
        "  (1, 3)\t1\n",
        "  (0, 4)\t1\n",
        "  (0, 5)\t1\n",
        "  (0, 6)\t1\n",
        "  (1, 7)\t1\n",
        "  (0, 8)\t1\n",
        "  (0, 9)\t1\n",
        "[[1 1 0 0 1 1 1 0 1 1]\n",
        " [2 0 1 1 0 0 0 1 0 0]]\n"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- Count vectors returned by `transform` are stored in the more memory efficient coordinate matrix format, we have to access the full standard vector for analysis though. \n",
      "\n",
      "Let's add some more data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "posts = ['how to open a beer without a bottle opener', \n",
      "           'Do girls like beer bottles or beer cans?',\n",
      "           'where did all my beer go?',\n",
      "           'where did all my beer go? where did all my beer go?',\n",
      "           'recycling beer bottles and cans',\n",
      "           'Is it worth recycling?',\n",
      "           'do not bring bottles to my backyard party, only cans please.', \n",
      "           'This is useless']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 71
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train = vect.fit_transform(posts)\n",
      "\n",
      "num_samples, num_features = X_train.shape\n",
      "\n",
      "print '#samples: {}, #features: {}'.format(num_samples, num_features)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "#samples: 8, #features: 31\n"
       ]
      }
     ],
     "prompt_number": 72
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- Unsurprisingly, we have 7 posts with a total of 31 different words. Now we can vectorize our data.\n",
      "\n",
      "Let's vectorize a new post, then see how similar it is to our existing corpus."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "new_post = 'Opening beer bottles and cans 101'\n",
      "new_post_vect = vect.transform([new_post])\n",
      "\n",
      "print(new_post_vect).toarray()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[0 1 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
       ]
      }
     ],
     "prompt_number": 73
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import scipy as sp\n",
      "\n",
      "def dists(v1, v2):\n",
      "    delta = v1-v2\n",
      "    # Calculate Euclidean \"norm\" distance\n",
      "    return sp.linalg.norm(delta.toarray())\n",
      "\n",
      "import sys\n",
      "\n",
      "def similarity(new_post_vector, corpus):\n",
      "    best_dist = 999\n",
      "    best_i = None\n",
      "    \n",
      "    for i in xrange(len(corpus.toarray())):\n",
      "        post = posts[i]\n",
      "        \n",
      "        if post == new_post:\n",
      "            continue\n",
      "        post_vec = corpus.getrow(i)\n",
      "        d = dists(post_vec, new_post_vector)\n",
      "        print 'Post %i with dist = %.2f: %s'%(i, d, post)\n",
      "        \n",
      "        if d < best_dist:\n",
      "            best_dist = d\n",
      "            best_i = i\n",
      "    print 'Best post is {} with dist = {}'.format(best_i, best_dist)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 74
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "similarity(new_post_vect, X_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Post 0 with dist = 3.00: how to open a beer without a bottle opener\n",
        "Post 1 with dist = 2.45: Do girls like beer bottles or beer cans?\n",
        "Post 2 with dist = 2.83: where did all my beer go?\n",
        "Post 3 with dist = 4.90: where did all my beer go? where did all my beer go?\n",
        "Post 4 with dist = 1.00: recycling beer bottles and cans\n",
        "Post 5 with dist = 2.83: Is it worth recycling?\n",
        "Post 6 with dist = 3.32: do not bring bottles to my backyard party, only cans please.\n",
        "Post 7 with dist = 2.65: This is useless\n",
        "Best post is 4 with dist = 1.0\n"
       ]
      }
     ],
     "prompt_number": 75
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Great, our first text similarity measurement! We can see here that post 3 is most similar to our new post. However, we can see that `post 2` is \"closer\" to `post 3`, even though `post 3` is simply `post 2` doubled. It is clear the simple counts of words is too simple. The next step is to normalize the word counts to get vectors of unitless lengths to avoid this problem."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Update our dists function\n",
      "def dists(v1, v2):\n",
      "    v1_norm = v1/sp.linalg.norm(v1.toarray())\n",
      "    v2_norm = v2/sp.linalg.norm(v2.toarray())\n",
      "    delta = v1_norm-v2_norm\n",
      "    # Calculate Euclidean \"norm\" distance\n",
      "    return sp.linalg.norm(delta.toarray())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 76
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "similarity(new_post_vect, X_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Post 0 with dist = 1.27: how to open a beer without a bottle opener\n",
        "Post 1 with dist = 0.86: Do girls like beer bottles or beer cans?\n",
        "Post 2 with dist = 1.26: where did all my beer go?\n",
        "Post 3 with dist = 1.26: where did all my beer go? where did all my beer go?\n",
        "Post 4 with dist = 0.46: recycling beer bottles and cans\n",
        "Post 5 with dist = 1.41: Is it worth recycling?\n",
        "Post 6 with dist = 1.18: do not bring bottles to my backyard party, only cans please.\n",
        "Post 7 with dist = 1.41: This is useless\n",
        "Best post is 4 with dist = 0.459505841095\n"
       ]
      }
     ],
     "prompt_number": 78
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Great, posts 2 & 3 are now equally similar to our new post."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Removing Less Important Words:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There are many words in language that do not carry much meaning in terms of the overall interpretation of the message. Words like \"it\" should be much less meaningful than \"beer\" in our current context. These less important words are called `stop words`, and can be removed from the posts since they do not help us distiguish between different posts."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Add english stop words to our vectorizer object.\n",
      "vect = CountVectorizer(min_df=1, stop_words='english')\n",
      "#Display a sample\n",
      "print sorted(vect.get_stop_words())[80:-150]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['empty', 'enough', 'etc', 'even', 'ever', 'every', 'everyone', 'everything', 'everywhere', 'except', 'few', 'fifteen', 'fify', 'fill', 'find', 'fire', 'first', 'five', 'for', 'former', 'formerly', 'forty', 'found', 'four', 'from', 'front', 'full', 'further', 'get', 'give', 'go', 'had', 'has', 'hasnt', 'have', 'he', 'hence', 'her', 'here', 'hereafter', 'hereby', 'herein', 'hereupon', 'hers', 'herself', 'him', 'himself', 'his', 'how', 'however', 'hundred', 'i', 'ie', 'if', 'in', 'inc', 'indeed', 'interest', 'into', 'is', 'it', 'its', 'itself', 'keep', 'last', 'latter', 'latterly', 'least', 'less', 'ltd', 'made', 'many', 'may', 'me', 'meanwhile', 'might', 'mill', 'mine', 'more', 'moreover', 'most', 'mostly', 'move', 'much', 'must', 'my', 'myself', 'name']\n"
       ]
      }
     ],
     "prompt_number": 92
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}